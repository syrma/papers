\section{Conclusion}
\label{sec:conc}
We have presented a general model for critics in Actor-Critic methods. We believe that this representation can become a basis to simplify the design of complex deep reinforcement learning agents.

We have provided a basic, practical example of exploiting the model to design a practical method and test it against currently challenging state-of-the-art control tasks.

As a future research perspective, we aim to provide a more complete set of experiments, in particular: 
\begin{itemize}
\item thoroughly investigate the range of possible hyperparameters and deduce a more general use case;
\item represent the aggregation function with different models, including trained nonlinear approximators;
\item encompass more known policy optimization methods and algorithms, and adapt our method accordingly.
\end{itemize}

In addition to these practical considerations, we intend to study the Generalized Critic model's shortcomings and provide more formal representations.



%Good for Hopper
% Meh for Humanoid
% bad for hc? smth bl3t
%It seems to be a potential optimization that some tasks can benefit from in the form of less variance and sometimes increased performance. More testing is needed on which tasks benefit, how can we predict which they are, and also the optimal parameters.