\section{Introduction}

In the recent years, technical advancement gave Artificial intelligence (AI) a new breath by facilitating the use of computation-heavy, highly scalable Artificial Neural Networks (ANN) as function approximators, thus achieving remarkable results in what is commonly referred to as Deep Learning. Indeed, despite their theoretical simplicity, ANNs have proved to be a reliable tool to approximate complex, real-world functions given enough computational power. Deep Reinforcement Learning refers to the practice of harnessing the combined power of new technologies and deep ANNs to put into practice the previously purely theoretical methods of Reinforcement Learning (RL). 

Deep RL subsequently achieved impressive results, in particular in board games such as Go, Shogi and Chess, leading to the algorithm largely outperforming any human player. In several more cases however, Deep RL offers more potential for future AI research than tangible state-of-the-art results. For instance, Reinforcement Learning methods have yet to be used in physical systems, either due to their needing far too much data to learn from %TODO (reference to some review of dqn or vpg)
or to their complexity and sensitivity to hyperparameters. %TODO (reference to drl that matters and similar stuff). 
While many applications are still more reliably solved with the help of human-expert-labeled data %ref
RL research offers the possibility of AI eventually growing beyond the need for such data, doing so by working on two fronts:
\begin{itemize}
\item Equipping AI with the autonomy to generate its own learning examples through interaction with the environment, instead of relying on expert-labeled data.
\item Developing a meta-paradigm that solves the problem of learning, irrespective of the task or application to be learned.
\end{itemize}

Relatively simple Reinforcement Learning algorithms have been limited in terms of performance and the complex tasks they can solve. However, by simply focusing on maximising performance to the exclusion of all other parameters, some research. Though results were immediately impressive, hindsight showed those results did not contribute as much as hoped to the field. Indeed, by finding more and more complex, specialised algorithms and optimizations, the very practical purpose of Reinforcement Learning as a general paradigm is lost sight of. The achievement of a very specilised algorithm solving a task at superhuman level fails to generalise to any other task. % in other words it's great that your super complex algorithm can beat all humans at starcraft, but like, that literally doesn't help me one bit to tackle any other problem

The purpose of Reinforcement Learning is to be reasonably simple and straightforward to implement so that it can be used in a variety of real life applications with little tweaking or human expertise required. The answer to performance issues therefore should not come at the cost of making implementation more complex and specialised.

Another problem in this direction a lot of research has taken is that implementation-level optimizations are often omitted when new methods are presented, presumably because they are considered to be unconsequential details. However, a closer examination of  major baseline implementations of RL algorithms show that not to be the case \cite{henderson2018deep}.

It is clear that much of documented results cannot be attributed purely to the methods as described in published literature.


In this paper, we shall be mindful of the aforementioned pitfalls of Reinforcement Learning research as we attempt to 