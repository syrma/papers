\section{Introduction}

In the recent years, technical advancement gave Artificial intelligence (AI) a new breath by facilitating the use of computation-heavy, highly scalable Artificial Neural Networks (ANN) as function approximators, thus achieving remarkable results in what is commonly referred to as Deep Learning. Indeed, despite their theoretical simplicity, ANNs have proved to be a reliable tool to approximate complex, real-world functions given enough computational power. Deep Reinforcement Learning refers to the practice of harnessing the combined power of new technologies and deep ANNs to put into practice the previously purely theoretical methods of Reinforcement Learning (RL). 

Deep RL subsequently achieved impressive results, in particular in board games such as Go, Shogi and Chess, leading to the algorithm largely outperforming any human player. In several more cases however, Deep RL offers more potential for future AI research than tangible state-of-the-art results. For instance, Reinforcement Learning methods have yet to be used in physical systems, either due to their needing far too much data to learn from %TODO (reference to some review of dqn or vpg)
or to their complexity and sensitivity to hyperparameters. %TODO (reference to drl that matters and similar stuff). 
While many applications are still more reliably solved with the help of human-expert-labeled data %ref
RL research offers the possibility of AI eventually growing beyond the need for such data, doing so by working on two fronts:
\begin{itemize}
\item Equipping AI with the autonomy to generate its own learning examples through interaction with the environment, instead of relying on expert-labeled data.
\item Developing a meta-paradigm that solves the problem of learning, irrespective of the task or application to be learned.
\end{itemize}
