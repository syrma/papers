\section{Introduction}
\label{sec:intro}
In the recent years, technical advancement gave Artificial intelligence (AI) a new breath by facilitating the use of computation-heavy, highly scalable Artificial Neural Networks (ANN) as function approximators, leading to remarkable achievements in computer vision \cite{krizhevsky2012imagenet}, speech recognition\cite{dahl2011context}, and virtually every field of AI. It has thus been shown that despite their theoretical simplicity, ANNs can be a reliable tool to approximate complex, real-world functions given enough computational power. %Such use of deep neural networks has been applied to R

%Reinforcement Learning (RL) is one field that highly benefitted from this application; using deep neural networks has enabled 

Deep Reinforcement Learning refers to the practice of combining the efficiency of modern hardware technologies with deep ANNs to put into practice the previously mostly theoretical field of Reinforcement Learning (RL). 

Deep RL subsequently achieved impressive results, in particular in board games such as Go, Shogi and Chess\cite{silver2018general}, leading to the algorithm largely outperforming any human player. Other applications have included playing games from pixel input like Atari\cite{mnih2015human} or Doom\cite{kempka2016vizdoom}, or simulating complex tasks for robotics\cite{levine2016end} and self-driving cars\cite{pan2017virtual}. In several more cases however, Deep RL offers more potential for future AI research than tangible state-of-the-art results. For instance, Reinforcement Learning methods are, due to their sample complexity and high sensitivity to noise\cite{henderson2018deep} \cite{engstrom2020implementation} \cite{dossa2021empirical}, generally not considered yet robust enough to safely use in physical systems, though a few successful attempts have been enabled by large-scale infrastructure and technical complexity. \cite{andrychowicz2020learning}

While many applications are still more reliably solved with the help of human-expert-labeled data,  RL research offers the possibility of AI eventually growing beyond the need for such data, doing so by working on two fronts:
\begin{itemize}
\item Equipping AI with the autonomy to generate its own learning examples through interaction with the environment, instead of relying on expert-labeled data.
\item Developing a meta-paradigm that solves the problem of learning, irrespective of the task or application to be learned.
\end{itemize}

Relatively simple RL algorithms have been limited in terms of performance and the scope of tasks they can solve. However, simply focusing on maximizing performance to the exclusion of all other parameters leads to the development of impractically complex methods that are hard to implement and tune.% 

The purpose of Reinforcement Learning is to be reasonably simple and straightforward to implement so that it can be used in a variety of real life applications with little tweaking or human expertise required.

Another problem in this direction a lot of research has taken is that implementation-level optimizations are often omitted when new methods are presented, presumably because they are considered to be inconsequential details. However, empirical study of  major baseline implementations of RL algorithms show that reported results are often impossible to reproduce without specific fine-tuning\cite{henderson2018deep}\cite{islam2017reproducibility}\cite{andrychowicz2021matters}\cite{dossa2021empirical}.


In this paper, we shall be mindful of the aforementioned pitfalls of RL research as we introduce a model whereby Policy Optimization algorithms can be expanded on and optimized. We also present our open-source sigle-file implementation for the Proximal Policy Optimization\cite{schulman2017proximal}(PPO) Deep RL algorithm and an implementation of our theoretical model on top of it. The paper will be structured as follows. In section 2, we quickly go over the fundamental definitions in RL this work builds on. We move on to present our theoretical model for expanding actor-critic RL algorithms with Ensemble methods in section 3. In section 4, we implement an example case on top of our single-file implementation of PPO. We run it over four simulated robotic environments and present our findings with regards to final performance and variance. We conclude the paper in the final section.