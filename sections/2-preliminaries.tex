\section{Preliminaries}
\label{sec:prelim}
This work builds on past theory on Reinforcement Learning and Policy Gradients. In this section, we will briefly summarize the essential structure of the RL problem and the categories of solutions relevant in the context of the present work.

\subsection{RL and Markov Decision Processes}

Reinforcement Learning is the field of Machine Learning concerned with decision making within an uncertain environment, acting upon it through actions with uncertain consequences, with the aim of achieving a specific goal while maximizing a numerical reward. A distinguishing feature of RL is that the agent interacts with the environment and pieces together a policy $\pi_theta$––the solution to the problem relying purely on the feedback of the environment with no input from human experts in any form. 

It is possible to model virtually any problem that corresponds to the description above using a mathematical framework called a Markov Decision Process (MDP.) 

The MDP is defined by the tuple ($\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \rho_0$) such that:
\begin{itemize}
\item $\mathcal{S}$ is the set of states, 
\item $\mathcal{A}$ is the set of actions, 
\item $\mathcal{P}: \mathcal{S}\times\mathcal{A}\times\mathcal{S}\mapsto \mathbb{R}$ is the transition probability distribution,
\item $r: \mathcal{S}\times\mathcal{A}\times\mathcal{S} \mapsto \mathbb{R}$ is the reward function,
\item $\rho_0:\mathcal{S}\mapsto\mathbb{R}$ is the initial state distribution.
\end{itemize}

\subsection{Policy Optimization}

For any reasonably complex RL task, environments are partially observable and the MDP cannot be solved directly. One of the possible ways to solve the RL problem is to estimate a policy through a stochastic gradient ascent algorithm. A commonly used estimator is as follows:

\[
g = \mathbb{E}
\left[\sum^{\infty}_{t=0} A^{\pi}(s_t, a_t) \nabla_{\theta}\log\pi_{\theta}(a_t | s_t) \right].
\]


Where $A^{\pi}(s_t, a_t)$ is the advantage function, which indicates for each action $a_t$ whether it is better or worse than the average action generated by the policy $\pi$ in a state $s_t$. It is formally defined as:

\[
A^\pi(s_t,a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t)
\]
for
\begin{align*}
&V(s_t) = \mathbb{E}\left[\sum_{l=0}^{\infty} r_l | s_0 = s_t \right],\\
&Q(s_t, a_t) = \mathbb{E}\left[\sum_{l=0}^{\infty} r_l | s_0 = s_t, a_0 = a_t \right]
\end{align*}

Though in practice, the advantage function is not known and is often replaced by an estimator $\hat{A}$ following one of several known estimation methods.

RL algorithms that estimate both a stochastic policy and a function that evaluates the current policy (such as the advantage function) are known as Actor/Critic algorithms. This is in reference to the function of each part: the current policy is responsible for generating the agent's next action (the actor) while the advantage function returns a positive or a negative value associated with each action (critic) thus influencing the update that will create the future policy.


\subsection{Proximal Policy Optimization}

Proximal Policy Optimization\cite{schulman2017proximal} (PPO) is a popular Actor/Critic Deep RL algorithm. It has since its creation been applied to a wide range of domains both in simulation and real life from robotics\cite{andrychowicz2020learning} to chip design\cite{mirhoseini2021graph}. It is a comparatively accessible algorithm to implement and deploy. A trend in model-free RL has been to improve performance through increasingly complex methods, which comes at the great cost of making RL methods hard to reproduce and validate as well as unpractical to use in physical systems. This conflicts with some of the stated goals of RL research, namely the aim to create a general paradigm that requires less human intervention than any other conventional AI paradigm. PPO balances ease of application with performance and is a good candidate for testing and improving generalization.

In PPO, the objective to maximize through gradient ascent is given as follows:

\[
L_{\text{PPO}} = \mathbb{E}\min\left(\frac{\pi(a_t | s_t)}{\pi_{\text{old}}(a_t|s_t)}\hat{A}^{\pi_{\text{old}}}_t, \text{clip}\left(\frac{\pi(a_t | s_t)}{\pi_\text{old}(a_t | s_t)}(a_t | s_t), 1 - \epsilon, 1 + \epsilon \right) \hat{A}^{\pi_{\text{old}}}_t \right)
\] 

where the ratio $\frac{\pi(a_t | s_t)}{\pi_{\text{old}}(a_t|s_t)}$ indicates how much more or less likely an action is to be taken according to the new policy compared to the old one. The clip function helps limit the size of the objective, with $\epsilon$ being the amount of clipping, so that we are less likely to take dramatic steps in updating the policy. Limiting the size of the update step is a characteristic feature of Trust-Region\cite{Schulman2015TrustRP} methods to make the policy improve monotonically or at least avoid drops in performance that may prevent the algorithm from converging which is one of the best known pitfalls of natural policy gradient methods.



