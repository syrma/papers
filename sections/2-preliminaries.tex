\section{Preliminaries}

We can distinguish three general directions that research takes when devising a solution to RL problems: optimizing the policy function, estimating a value function, and representing the model. We tend to classify solutions by whether they include or omit one or more of the above-mentioned elements.

For instance, model-free RL is a class of methods whose approach to solving the RL problem does not rely on having a model of the environment: that is, an internal representation (given or approximated) of the reward function and the transition probability function. A popular sub-class of model-free RL are the Policy Gradient methods.

As their name suggests, Policy Gradient methods mainly try to solve the RL problem directly by optimizing the policy function $\pi$. They are relatively straight-forward to implement 


PPO is relatively simple to implement and deploy. The trend in model-free RL has been to improve performance through increasingly complex methods, which comes at the great cost of the making RL methods unpractical to use in physical systems. This is at odds with the very purpose of advancing Reinforcement Learning research, a field aiming to create a general paradigm that requires less human intervention than any other known Artificial Intelligence model.

The present article aims to introduce a model whereby Policy Optimization algorithms can be expanded and 

